{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7acedee2-103d-407e-8d2f-a2959ba30b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import transformers\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, DataLoader\n",
    "from transformers import (\n",
    "    RobertaModel,\n",
    "    RobertaTokenizerFast,\n",
    "    RobertaForSequenceClassification, TrainingArguments, Trainer\n",
    ")\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch import nn\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3bb94a-588f-4978-b2c8-fd026cbf1709",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_templates = {\n",
    "    1: [\n",
    "        \"concept <con> can be described as <prop_list>.\",\n",
    "        \"concept <con> can be described as <predict_prop>.\",\n",
    "    ],\n",
    "    2: [\n",
    "        \"concept <con> can be described as <prop_list>?\",\n",
    "        \"<[MASK]>, concept <con> can be described as <predict_prop>.\",\n",
    "    ],\n",
    "    3: [\n",
    "        \"concept <con> can be described as <predict_prop>?\",\n",
    "        \"<[MASK]>, concept <con> can be described as <prop_list>.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def preprocess_dataset(data_df, tokenizer, context_id = 3):\n",
    "    \n",
    "    print (f\"Mask token : {tokenizer.mask_token}\")\n",
    "    print (f\"Mask token ID : {tokenizer.mask_token_id}\")\n",
    "\n",
    "    def preprocess_conjuct_prop (conjuct_props):\n",
    "\n",
    "        if conjuct_props == \"no_similar_property\":\n",
    "            conjuct_props = \"\"\n",
    "        else:\n",
    "\n",
    "            conjuct_props = conjuct_props.split(\", \")\n",
    "\n",
    "            if len(conjuct_props) >= 2:\n",
    "\n",
    "                conjuct_props[-1] = \"and \" + conjuct_props[-1]\n",
    "                conjuct_props = \", \".join(conjuct_props)\n",
    "            else:\n",
    "                conjuct_props = \", \".join(conjuct_props)\n",
    "\n",
    "        return conjuct_props\n",
    "\n",
    "\n",
    "    data_df[\"conjuct_prop\"] = data_df[\"conjuct_prop\"].apply(preprocess_conjuct_prop)\n",
    "\n",
    "    # print (data_df.head(n=20))\n",
    "\n",
    "    if context_id is not None:\n",
    "\n",
    "        sent_1_template, sent_2_template = context_templates[context_id]\n",
    "\n",
    "        print (\"sent_1_template :\", sent_1_template)\n",
    "        print (\"sent_2_template :\", sent_2_template)\n",
    "\n",
    "\n",
    "    def get_sent_1(template, concept, predict_prop):\n",
    "\n",
    "        text = template.replace(\"<con>\", concept).replace(\"<predict_prop>\", predict_prop)\n",
    "        return text\n",
    "\n",
    "    def get_sent_2(template, concept, conjuct_props):\n",
    "\n",
    "        text = template.replace(\"<[MASK]>\", tokenizer.mask_token).replace(\"<con>\", concept).replace(\"<prop_list>\", conjuct_props)\n",
    "        return text\n",
    "\n",
    "    data_df[\"sent_1\"] = data_df.apply(lambda x : get_sent_1(sent_1_template, x[\"concept\"], x[\"predict_prop\"]), axis=1)\n",
    "    data_df[\"sent_2\"] = data_df.apply(lambda x : get_sent_2(sent_2_template, x[\"concept\"], x[\"conjuct_prop\"]), axis=1)\n",
    "\n",
    "    # print (self.data_df[[\"sent_1\", \"sent_2\"]].head(n=20))\n",
    "\n",
    "    print(data_df[\"sent_1\"], data_df[\"sent_2\"], data_df[\"labels\"])\n",
    "\n",
    "    return data_df\n",
    "\n",
    "class ConceptPropertyAugmentationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_file, max_len = 60, context_id=3):\n",
    "        \n",
    "        self.data_df = pd.read_csv(\n",
    "                    data_file,\n",
    "                    sep=\"\\t\",\n",
    "                    header=None,\n",
    "                    names=[\"concept\", \"conjuct_prop\", \"predict_prop\", \"labels\"],\n",
    "                    dtype={\n",
    "                        \"concept\": str,\n",
    "                        \"conjuct_prop\": str,\n",
    "                        \"predict_prop\": str,\n",
    "                        \"labels\": float,\n",
    "                    },\n",
    "                )[0:100]\n",
    "\n",
    "        \n",
    "        self.tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "        self.max_len = max_len\n",
    "        self.context_id = context_id\n",
    "        self.mask_token = self.tokenizer.mask_token\n",
    "        \n",
    "        self.data_df = preprocess_dataset(data_df=self.data_df, tokenizer=self.tokenizer, context_id=self.context_id)\n",
    "        \n",
    "        # print (self.data_df)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return (len(self.data_df))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \n",
    "        sent_1 = self.data_df[\"sent_1\"][idx]\n",
    "        sent_2 = self.data_df[\"sent_2\"][idx]\n",
    "        \n",
    "        labels = self.data_df[\"labels\"][idx]\n",
    "        \n",
    "        encoded_dict = self.tokenizer.encode_plus(\n",
    "            text=sent_1,\n",
    "            text_pair=sent_2,\n",
    "            max_length=self.max_len,\n",
    "            add_special_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "\n",
    "        \n",
    "        encoded_dict[\"labels\"] = torch.tensor(labels)\n",
    "        print (encoded_dict)\n",
    "        \n",
    "        print (encoded_dict[\"input_ids\"].shape)\n",
    "        print (encoded_dict[\"attention_mask\"].shape)\n",
    "        print (encoded_dict[\"labels\"].shape)\n",
    "                \n",
    "        return encoded_dict\n",
    "    \n",
    "\n",
    "\n",
    "# train_file = \"\"\n",
    "valid_file = \"/home/amitgajbhiye/Downloads/embeddings_con_prop/deberta_nli_predict_prop_similar/sim3_deberta_nli_predict_prop_similar_5_neg_valid_mscg_cnetp.tsv\"\n",
    "\n",
    "valid_data = ConceptPropertyAugmentationDataset(data_file = valid_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424d8c98-747c-47e4-b805-ea413dacf221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c235e3-1b46-4ab6-bc24-37b413afd076",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPropConjuctionJoint(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ModelPropConjuctionJoint, self).__init__()\n",
    "\n",
    "\n",
    "        # self.encoder = RobertaModel.from_pretrained(\"MOdel Path\")\n",
    "        self.encoder = AutoModel.from_pretrained(\"roberta-base\")\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.encoder.config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(self.encoder.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        \n",
    "        \n",
    "        input_ids = input_ids.squeeze()\n",
    "        attention_mask = attention_mask.squeeze()\n",
    "        \n",
    "        \n",
    "        print (f\"input_ids : {input_ids.shape}\")\n",
    "        print (f\"attention_mask : {attention_mask.shape}\")\n",
    "        print (f\"labels : {labels.shape}\")\n",
    "\n",
    "        loss_fct = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        output = self.encoder(\n",
    "            input_ids=input_ids.squeeze(),\n",
    "            attention_mask=attention_mask.squeeze())\n",
    "        \n",
    "        hidden_states = output.last_hidden_state\n",
    "        \n",
    "        print (f\"hidden_states : {hidden_states.shape}\")\n",
    "\n",
    "        def get_mask_token_embeddings(last_layer_hidden_states):\n",
    "\n",
    "            MASK_TOKEN_ID = 50264\n",
    "\n",
    "            _, mask_token_index = (\n",
    "                input_ids == torch.tensor(MASK_TOKEN_ID)\n",
    "            ).nonzero(as_tuple=True)\n",
    "\n",
    "            mask_vectors = torch.vstack(\n",
    "                [\n",
    "                    torch.index_select(v, 0, torch.tensor(idx))\n",
    "                    for v, idx in zip(last_layer_hidden_states, mask_token_index)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            return mask_vectors\n",
    "\n",
    "        mask_vectors = get_mask_token_embeddings(\n",
    "            last_layer_hidden_states=hidden_states\n",
    "        )\n",
    "\n",
    "        mask_vectors = self.dropout(mask_vectors)\n",
    "        mask_logits = self.classifier(mask_vectors).view(-1)\n",
    "        \n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.view(-1).float()\n",
    "            loss = loss_fct(mask_logits, labels)\n",
    "\n",
    "        print (\"loss :\", loss)\n",
    "        \n",
    "        return (loss, mask_logits, mask_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7a717e-db3f-4442-9371-308a98b42ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"accuracy\"\n",
    "model_name = \"dummy_roberta_base\"\n",
    "batch_size = 16\n",
    "\n",
    "model = ModelPropConjuctionJoint()\n",
    "\n",
    "args = TrainingArguments(output_dir = \".\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=valid_data,\n",
    "    eval_dataset=None,\n",
    "    tokenizer=None,\n",
    "    compute_metrics=None\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f453d12-d15d-490a-aa8c-33da3e2ab13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a361332c-e520-4799-a5a8-41324e3fe467",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "316b988d-02f3-455c-a18b-c76f364d0ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e95cfff-a5a7-40b7-af8d-70bddded1240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0756a515-5b6a-4bcb-a32e-1c87fa5dea05",
   "metadata": {
    "tags": []
   },
   "source": [
    "context_templates = {\n",
    "    1: [\n",
    "        \"concept <con> can be described as <prop_list>.\",\n",
    "        \"concept <con> can be described as <predict_prop>.\",\n",
    "    ],\n",
    "    2: [\n",
    "        \"concept <con> can be described as <prop_list>?\",\n",
    "        \"<[MASK]>, concept <con> can be described as <predict_prop>.\",\n",
    "    ],\n",
    "    3: [\n",
    "        \"concept <con> can be described as <predict_prop>?\",\n",
    "        \"<[MASK]>, concept <con> can be described as <prop_list>.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def preprocess_dataset (concept_property_file, context_id = None):\n",
    "\n",
    "    data_df = pd.read_csv(\n",
    "                concept_property_file,\n",
    "                sep=\"\\t\",\n",
    "                header=None,\n",
    "                names=[\"concept\", \"conjuct_prop\", \"predict_prop\", \"labels\"],\n",
    "                dtype={\n",
    "                    \"concept\": str,\n",
    "                    \"conjuct_prop\": str,\n",
    "                    \"predict_prop\": str,\n",
    "                    \"labels\": int,\n",
    "                },\n",
    "            )\n",
    "\n",
    "    print (f\"Mask Token : {tokenizer.mask_token}\")\n",
    "\n",
    "    # print (data_df.head(n=20))\n",
    "\n",
    "    def preprocess_conjuct_prop (conjuct_props):\n",
    "\n",
    "        if conjuct_props == \"no_similar_property\":\n",
    "            conjuct_props = \"\"\n",
    "        else:\n",
    "\n",
    "            conjuct_props = conjuct_props.split(\", \")\n",
    "\n",
    "            if len(conjuct_props) >= 2:\n",
    "\n",
    "                conjuct_props[-1] = \"and \" + conjuct_props[-1]\n",
    "                conjuct_props = \", \".join(conjuct_props)\n",
    "            else:\n",
    "                conjuct_props = \", \".join(conjuct_props)\n",
    "\n",
    "        return conjuct_props\n",
    "\n",
    "\n",
    "    data_df[\"conjuct_prop\"] = data_df[\"conjuct_prop\"].apply(preprocess_conjuct_prop)\n",
    "\n",
    "    # print (data_df.head(n=20))\n",
    "\n",
    "    if context_id is not None:\n",
    "\n",
    "        sent_1_template, sent_2_template = context_templates[context_id]\n",
    "\n",
    "        print (\"sent_1_template :\", sent_1_template)\n",
    "        print (\"sent_2_template :\", sent_2_template)\n",
    "\n",
    "\n",
    "    def get_sent_1(template, concept, predict_prop):\n",
    "        text = template.replace(\"<con>\", concept).replace(\"<predict_prop>\", predict_prop)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def get_sent_2(template, concept, conjuct_props):\n",
    "        text = template.replace(\"<[MASK]>\", tokenizer.mask_token).replace(\"<con>\", concept).replace(\"<prop_list>\", conjuct_props)\n",
    "\n",
    "        return text\n",
    "\n",
    "    data_df[\"sent_1\"] = data_df.apply(lambda x : get_sent_1(sent_1_template, x[\"concept\"], x[\"predict_prop\"]), axis=1)\n",
    "    data_df[\"sent_2\"] = data_df.apply(lambda x : get_sent_2(sent_2_template, x[\"concept\"], x[\"conjuct_prop\"]), axis=1)\n",
    "\n",
    "    print (data_df[[\"sent_1\", \"sent_2\"]].head(n=20))\n",
    "\n",
    "    return data_df[\"sent_1\"], data_df[\"sent_2\"], data_df[\"labels\"]\n",
    "\n",
    "    \n",
    "\n",
    "valid_file = \"/home/amitgajbhiye/Downloads/embeddings_con_prop/deberta_nli_predict_prop_similar/sim3_deberta_nli_predict_prop_similar_5_neg_valid_mscg_cnetp.tsv\"\n",
    "\n",
    "sent_1, sent_2, labels = preprocess_dataset(valid_file, context_id=3)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6d1bd6-9e91-43d7-b30b-d7124e6e81e6",
   "metadata": {},
   "source": [
    "sent_1[0], sent_2[0], labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68569ac1-5531-413b-b831-3b69d0ff59ef",
   "metadata": {},
   "source": [
    "def preprocess_function(sent_1, sent_2):\n",
    "    return tokenizer(sent_1, sent_2, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267ac237-2a98-4d71-b308-070d34023b61",
   "metadata": {},
   "source": [
    "tok = preprocess_function(sent_1[0], sent_2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30baefda-7f7c-4eb4-bb72-43388282d194",
   "metadata": {},
   "source": [
    "tok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef08e0df-469c-4158-af44-c04974ae61c3",
   "metadata": {},
   "source": [
    "tokenizer.name_or_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f99366-b00f-4b98-bfe9-a19d7060e13b",
   "metadata": {},
   "source": [
    "\"roberta\" in tokenizer.name_or_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f29af-8361-4fff-996b-6ebac2f90c22",
   "metadata": {},
   "source": [
    "inp = tokenizer.encode_plus(sent_1[0], sent_2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb722a1c-f90b-4218-8900-269ba89a75fb",
   "metadata": {},
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db3107e-2566-4c05-a2a3-99e18fc3f394",
   "metadata": {},
   "source": [
    "\"roberta\" in model.name_or_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2286da2b-6c62-4766-8270-e3281047f5df",
   "metadata": {},
   "source": [
    "model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
