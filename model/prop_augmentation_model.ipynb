{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7acedee2-103d-407e-8d2f-a2959ba30b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification\n",
    "\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3bb94a-588f-4978-b2c8-fd026cbf1709",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_templates = {\n",
    "    1: [\n",
    "        \"concept <con> can be described as <prop_list>.\",\n",
    "        \"concept <con> can be described as <predict_prop>.\",\n",
    "    ],\n",
    "    2: [\n",
    "        \"concept <con> can be described as <prop_list>?\",\n",
    "        \"<[MASK]>, concept <con> can be described as <predict_prop>.\",\n",
    "    ],\n",
    "    3: [\n",
    "        \"concept <con> can be described as <predict_prop>?\",\n",
    "        \"<[MASK]>, concept <con> can be described as <prop_list>.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "\n",
    "\n",
    "def preprocess_dataset (concept_property_file, context_id = None):\n",
    "    \n",
    "    data_df = pd.read_csv(\n",
    "                concept_property_file,\n",
    "                sep=\"\\t\",\n",
    "                header=None,\n",
    "                names=[\"concept\", \"conjuct_prop\", \"predict_prop\", \"labels\"],\n",
    "                dtype={\n",
    "                    \"concept\": str,\n",
    "                    \"conjuct_prop\": str,\n",
    "                    \"predict_prop\": str,\n",
    "                    \"labels\": int,\n",
    "                },\n",
    "            )\n",
    "    \n",
    "    print (f\"Mask Token : {tokenizer.mask_token}\")\n",
    "    \n",
    "    # print (data_df.head(n=20))\n",
    "    \n",
    "    def preprocess_conjuct_prop (conjuct_props):\n",
    "        \n",
    "        if conjuct_props == \"no_similar_property\":\n",
    "            conjuct_props = \"\"\n",
    "        else:\n",
    "            \n",
    "            conjuct_props = conjuct_props.split(\", \")\n",
    "\n",
    "            if len(conjuct_props) >= 2:\n",
    "\n",
    "                conjuct_props[-1] = \"and \" + conjuct_props[-1]\n",
    "                conjuct_props = \", \".join(conjuct_props)\n",
    "            else:\n",
    "                conjuct_props = \", \".join(conjuct_props)\n",
    "        \n",
    "        return conjuct_props\n",
    "    \n",
    "    \n",
    "    data_df[\"conjuct_prop\"] = data_df[\"conjuct_prop\"].apply(preprocess_conjuct_prop)\n",
    "\n",
    "    # print (data_df.head(n=20))\n",
    "    \n",
    "    if context_id is not None:\n",
    "        \n",
    "        sent_1_template, sent_2_template = context_templates[context_id]\n",
    "        \n",
    "        print (\"sent_1_template :\", sent_1_template)\n",
    "        print (\"sent_2_template :\", sent_2_template)\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_sent_1(template, concept, predict_prop):\n",
    "        text = template.replace(\"<con>\", concept).replace(\"<predict_prop>\", predict_prop)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def get_sent_2(template, concept, conjuct_props):\n",
    "        text = template.replace(\"<[MASK]>\", tokenizer.mask_token).replace(\"<con>\", concept).replace(\"<prop_list>\", conjuct_props)\n",
    "            \n",
    "        return text\n",
    "        \n",
    "    data_df[\"sent_1\"] = data_df.apply(lambda x : get_sent_1(sent_1_template, x[\"concept\"], x[\"predict_prop\"]), axis=1)\n",
    "    data_df[\"sent_2\"] = data_df.apply(lambda x : get_sent_2(sent_2_template, x[\"concept\"], x[\"conjuct_prop\"]), axis=1)\n",
    "    \n",
    "    print (data_df[[\"sent_1\", \"sent_2\"]].head(n=20))\n",
    "    \n",
    "    return data_df[\"sent_1\"], data_df[\"sent_2\"], data_df[\"labels\"]\n",
    "    \n",
    "    \n",
    "\n",
    "valid_file = \"/home/amitgajbhiye/Downloads/embeddings_con_prop/deberta_nli_predict_prop_similar/sim3_deberta_nli_predict_prop_similar_5_neg_valid_mscg_cnetp.tsv\"\n",
    "\n",
    "sent_1, sent_2, labels = preprocess_dataset(valid_file, context_id=3)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce32deb-4a5b-464a-88ef-46b776e96383",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1[0], sent_2[0], labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdb17d4-d682-40f2-b6c1-8b801ee1278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(sent_1, sent_2):\n",
    "    return tokenizer(sent_1, sent_2, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b92593f-a2d1-4ed1-871a-f53b90734e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = preprocess_function(sent_1[0], sent_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b2726-5c90-4d1e-8ae2-2d138f4d64c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7e541b-5092-483b-991d-7c41b4e039a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c5fafe-0049-4df5-bb7b-8e39993a2c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"roberta\" in tokenizer.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c670ffc-9756-490d-b1ab-0217a8d72615",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tokenizer.encode_plus(sent_1[0], sent_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f7395-2fc3-4ab6-a375-362b9919af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e3a660-b5db-4d01-98b4-d34855f94d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"roberta\" in model.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71357d3-385f-465e-a449-7448bb2dbc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
