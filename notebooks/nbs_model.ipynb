{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22433541-03ce-4687-8096-7b790c99e778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from transformers import BertModel,BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bbe256-4cb3-43cd-9810-248b2c3c6813",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d18be69-d000-48b7-81cf-575e2acfaa8e",
   "metadata": {},
   "source": [
    "## Dot Product Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11806e43-69b1-4583-bee5-654a9ba9831d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class ConceptPropertyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConceptPropertyModel, self).__init__()\n",
    "\n",
    "        # self._concept_encoder = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        # self._property_encoder = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        self._concept_encoder = BertModel.from_pretrained(\"/scratch/c.scmag3/conceptEmbeddingModel/bertBaseUncasedPreTrained\")\n",
    "        self._property_encoder = BertModel.from_pretrained(\"/scratch/c.scmag3/conceptEmbeddingModel/bertBaseUncasedPreTrained\")\n",
    "\n",
    "        self.dropout_prob = 0.2\n",
    "        self.strategy = \"mean\"\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        concept_input_id,\n",
    "        concept_attention_mask,\n",
    "        property_input_id,\n",
    "        property_attention_mask,\n",
    "    ):\n",
    "\n",
    "        concept_output = self._concept_encoder(\n",
    "            input_ids=concept_input_id, attention_mask=concept_attention_mask\n",
    "        )\n",
    "\n",
    "        property_output = self._property_encoder(\n",
    "            input_ids=property_input_id, attention_mask=property_attention_mask\n",
    "        )\n",
    "\n",
    "        concept_last_hidden_states, concept_cls = (\n",
    "            concept_output.get(\"last_hidden_state\"),\n",
    "            concept_output.get(\"pooler_output\"),\n",
    "        )\n",
    "\n",
    "        property_last_hidden_states, property_cls = (\n",
    "            property_output.get(\"last_hidden_state\"),\n",
    "            property_output.get(\"pooler_output\"),\n",
    "        )\n",
    "\n",
    "        if self.strategy == \"mean\":\n",
    "\n",
    "            # The dot product of the average of the last hidden states of the concept and property hidden states.\n",
    "\n",
    "            v_concept_avg = torch.sum(\n",
    "                concept_last_hidden_states\n",
    "                * concept_attention_mask.unsqueeze(1).transpose(2, 1),\n",
    "                dim=1,\n",
    "            ) / torch.sum(concept_attention_mask, dim=1, keepdim=True)\n",
    "\n",
    "            # Normalising concept vectors\n",
    "            v_concept_avg = normalize(v_concept_avg, p=2, dim=1)\n",
    "\n",
    "            v_property_avg = torch.sum(\n",
    "                property_last_hidden_states\n",
    "                * property_attention_mask.unsqueeze(1).transpose(2, 1),\n",
    "                dim=1,\n",
    "            ) / torch.sum(property_attention_mask, dim=1, keepdim=True)\n",
    "\n",
    "            logits = (\n",
    "                (v_concept_avg * v_property_avg)\n",
    "                .sum(-1)\n",
    "                .reshape(v_concept_avg.shape[0], 1)\n",
    "            )\n",
    "\n",
    "            return v_concept_avg, v_property_avg, logits\n",
    "\n",
    "        elif self.strategy == \"cls\":\n",
    "            # The dot product of concept property cls vectors.\n",
    "\n",
    "            # Normalising concept vectors\n",
    "            concept_cls = normalize(concept_cls, p=2, dim=1)\n",
    "\n",
    "            logits = (concept_cls * property_cls).sum(-1).reshape(concept_cls.shape[0], 1)\n",
    "\n",
    "            return concept_cls, property_cls, logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ba5eb2-0edb-494d-af08-a5334d7cee86",
   "metadata": {},
   "source": [
    "## Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb0e8a5-dc8d-49de-bf37-fd3286306093",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_train = \"mscg_train_pos.tsv\"\n",
    "file_valid = \"mscg_valid_pos.tsv\"\n",
    "\n",
    "# file_train = \"mscg_test_pos.tsv\"\n",
    "# file_valid = \"mscg_test_pos.tsv\"\n",
    "\n",
    "context_num = 3\n",
    "best_model_path = \"3_cntx_best_model.pt\"\n",
    "\n",
    "\n",
    "num_epoch = 100\n",
    "bs = 32\n",
    "early_stopping_patience = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcacc903-ebe1-494f-af7d-fc1285184d6f",
   "metadata": {},
   "source": [
    "## Datasets Class to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03abac47-c760-4bf3-b499-1d73b421ee55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConceptPropertyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_file_path):\n",
    "\n",
    "        self.data_df = pd.read_csv(data_file_path,\n",
    "            sep=\"\\t\",\n",
    "            header=None,\n",
    "            names=[\"concept\", \"property\"],\n",
    "        )\n",
    "        \n",
    "        # self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"/scratch/c.scmag3/conceptEmbeddingModel/bertBaseUncasedPreTrained/tokenizer\")\n",
    "\n",
    "        # self.concepts_unique = self.data_df[\"concept\"].unique()\n",
    "        # self.properties_unique = self.data_df[\"property\"].unique()\n",
    "\n",
    "        self.concept2idx, self.idx2concept = self.create_concept_idx_dicts()\n",
    "        self.property2idx, self.idx2property = self.create_property_idx_dicts()\n",
    "\n",
    "        self.con_pro_dict, self.prop_con_dict = self.populate_dict()\n",
    "        \n",
    "        # print (\"self.con_pro_dict :\", self.con_pro_dict)\n",
    "        \n",
    "        self.context_num = context_num\n",
    "        \n",
    "    def create_concept_idx_dicts(self):\n",
    "        \n",
    "        unique_concepts = self.data_df[\"concept\"].unique()\n",
    "        \n",
    "        item2idx, idx2item = {}, {}\n",
    "        \n",
    "        for idx, item in enumerate(unique_concepts):\n",
    "            item2idx[item] = idx\n",
    "            idx2item[idx] = item\n",
    "\n",
    "        return item2idx, idx2item\n",
    "\n",
    "    def create_property_idx_dicts(self):\n",
    "        \n",
    "        unique_properties = self.data_df[\"property\"].unique()\n",
    "        \n",
    "        item2idx, idx2item = {}, {}\n",
    "        \n",
    "        for idx, item in enumerate(unique_properties):\n",
    "            item2idx[item] = idx\n",
    "            idx2item[idx] = item\n",
    "\n",
    "        return item2idx, idx2item\n",
    "\n",
    "    def populate_dict(self):\n",
    "        \n",
    "        concept_property_dict, property_concept_dict = {}, {}\n",
    "        \n",
    "        unique_concepts = self.data_df[\"concept\"].unique()\n",
    "        unique_properties = self.data_df[\"property\"].unique()\n",
    "        \n",
    "        self.data_df.set_index(\"concept\", inplace=True)\n",
    "        \n",
    "        for concept in unique_concepts:\n",
    "            \n",
    "            concept_id = self.concept2idx[concept]\n",
    "            \n",
    "            property_list = self.data_df.loc[concept].values.flatten()\n",
    "            property_ids = np.asarray([self.property2idx[x] for x in property_list])\n",
    "            \n",
    "            concept_property_dict[concept_id] = property_ids\n",
    "        \n",
    "        self.data_df.reset_index(inplace=True)\n",
    "        \n",
    "        self.data_df.set_index(\"property\", inplace=True)\n",
    "        \n",
    "        for prop in unique_properties:\n",
    "            \n",
    "            property_id = self.property2idx[prop]\n",
    "            \n",
    "            concept_list = self.data_df.loc[prop].values.flatten()\n",
    "            concept_ids = np.asarray([self.concept2idx[x] for x in concept_list])\n",
    "            \n",
    "            property_concept_dict[property_id] = concept_ids\n",
    "        \n",
    "        self.data_df.reset_index(inplace=True)\n",
    "        \n",
    "        return concept_property_dict, property_concept_dict\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.data_df[\"concept\"][idx], self.data_df[\"property\"][idx]\n",
    "    \n",
    "    def add_context(self, batch):\n",
    "        \n",
    "        if self.context_num == 4:\n",
    "            \n",
    "            concept_context = \"Yesterday, I saw another \"\n",
    "            property_context = \"Yesterday, I saw a thing which is \"\n",
    "            \n",
    "            concepts_batch = [concept_context + x + \".\" for x in batch[0]]\n",
    "            property_batch = [property_context + x + \".\" for x in batch[1]]\n",
    "            \n",
    "        elif self.context_num == 1:\n",
    "            \n",
    "            concept_context = \"Concept : \"\n",
    "            property_context = \"Property : \"\n",
    "            \n",
    "            concepts_batch = [concept_context + x + \".\" for x in batch[0]]\n",
    "            property_batch = [property_context + x + \".\" for x in batch[1]]\n",
    "                    \n",
    "        elif self.context_num == 2:\n",
    "            \n",
    "            concept_context = \"The notion we are modelling : \"\n",
    "            property_context = \"The notion we are modelling : \"\n",
    "            \n",
    "            concepts_batch = [concept_context + x + \".\" for x in batch[0]]\n",
    "            property_batch = [property_context + x + \".\" for x in batch[1]]\n",
    "            \n",
    "            \n",
    "        elif self.context_num == 3:\n",
    "            \n",
    "            prefix_num = 5\n",
    "            suffix_num = 4  \n",
    "            \n",
    "            concepts_batch = [\"[MASK] \" * prefix_num + concept + \" \" + \"[MASK] \" * suffix_num + \".\" for concept in batch[0]]\n",
    "            property_batch = [\"[MASK] \" * prefix_num + prop + \" \" + \"[MASK] \" * suffix_num + \".\" for prop in batch[1]]\n",
    "            \n",
    "        \n",
    "        elif self.context_num == 5:\n",
    "            \n",
    "            concept_context = \"The notion we are modelling is called \"\n",
    "            property_context = \"The notion we are modelling is called \"\n",
    "            \n",
    "            concepts_batch = [concept_context + x + \".\" for x in batch[0]]\n",
    "            property_batch = [property_context + x + \".\" for x in batch[1]]\n",
    "                    \n",
    "        \n",
    "        return concepts_batch, property_batch\n",
    "        \n",
    "    def tokenize(self, concept_batch, property_batch, concept_max_len=64, property_max_len=64):\n",
    "        \n",
    "        concept_ids = self.tokenizer(\n",
    "            concept_batch,\n",
    "            max_length=concept_max_len,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        property_ids = self.tokenizer(\n",
    "            property_batch,\n",
    "            max_length=property_max_len,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "                \n",
    "        return {\n",
    "            \"concept_inp_id\": concept_ids.get(\"input_ids\"),\n",
    "            \"concept_atten_mask\": concept_ids.get(\"attention_mask\"),\n",
    "            \"property_inp_id\": property_ids.get(\"input_ids\"),\n",
    "            \"property_atten_mask\": property_ids.get(\"attention_mask\")}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e084478-13ca-4fc4-a5d9-3e96d4f1b47d",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7fd686-33f5-498b-9105-d83d2a2ef800",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = ConceptPropertyDataset(file_train)\n",
    "train_data_sampler = RandomSampler(train_dataset)\n",
    "train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=bs, sampler=train_data_sampler)\n",
    "\n",
    "valid_dataset = ConceptPropertyDataset(file_valid)\n",
    "valid_sampler = RandomSampler(valid_dataset)\n",
    "valid_dl = torch.utils.data.DataLoader(valid_dataset, batch_size=bs, sampler=valid_sampler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a580739a-52e0-4671-9ded-0d72258fe501",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab7d0c6-8d8f-4acf-a3b3-65a45fa62460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = ConceptPropertyModel()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce908bb-d0cb-4d2c-b328-dfc88dac048d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=2e-6)\n",
    "\n",
    "# warmup_steps = math.ceil(len(train_dl) * num_epoch * 0.1)  # 10% of train data for warm-up\n",
    "warmup_steps = 0\n",
    "\n",
    "total_training_steps = (\n",
    "        len(train_dl) * num_epoch\n",
    "    )\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_training_steps,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b2432f-ae7d-4452-b4fb-9c1e5163c894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(labels, preds):\n",
    "\n",
    "    assert len(labels) == len(\n",
    "        preds\n",
    "    ), f\"labels len: {len(labels)} is not equal to preds len {len(preds)}\"\n",
    "\n",
    "    scores = {\n",
    "        \"binary_f1\": round(f1_score(labels, preds, average=\"binary\"), 4),\n",
    "        \"micro_f1\": round(f1_score(labels, preds, average=\"micro\"), 4),\n",
    "        \"macro_f1\": round(f1_score(labels, preds, average=\"macro\"), 4),\n",
    "        \"weighted_f1\": round(f1_score(labels, preds, average=\"weighted\"), 4),\n",
    "        \"accuracy\": round(accuracy_score(labels, preds), 4),\n",
    "        \"classification report\": classification_report(labels, preds, labels=[0, 1]),\n",
    "        \"confusion matrix\": confusion_matrix(labels, preds, labels=[0, 1]),\n",
    "    }\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7457807-f8d7-49ec-bdda-0baaf0efc42f",
   "metadata": {},
   "source": [
    "## Loss Calculation with in-batch negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23afb989-18c2-4327-bf58-61ce3f2c1623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(dataset, batch, concept_embedding, property_embedding, logits, device=device):\n",
    "    \n",
    "    # self.concept2idx, self.idx2concept = self.create_concept_idx_dicts()\n",
    "    # self.property2idx, self.idx2property = self.create_property_idx_dicts()\n",
    "    \n",
    "    # print (\"con_pro_dict :\", dataset.con_pro_dict, \"\\n\")\n",
    "    \n",
    "    num_neg_concept = random.randint(0, concept_embedding.shape[0])\n",
    "    \n",
    "    # print (\"\\t  num_neg_concept :\", num_neg_concept, flush=True)\n",
    "    \n",
    "    batch_logits, batch_labels = [], []\n",
    "    \n",
    "    concept_id_list_for_batch = torch.tensor([dataset.concept2idx[concept] for concept in batch[0]], device=device)\n",
    "    property_id_list_for_batch = torch.tensor([dataset.property2idx[prop] for prop in batch[1]], device=device)\n",
    "    \n",
    "    # print (\"concept_id_list_for_batch :\", concept_id_list_for_batch)\n",
    "    # print (\"property_id_list_for_batch :\", property_id_list_for_batch)\n",
    "        \n",
    "    # neg_concept_list, neg_property_list = [], []\n",
    "    \n",
    "    logits_pos_concepts = ((concept_embedding * property_embedding).sum(-1).reshape(concept_embedding.shape[0], 1))\n",
    "    labels_pos_concepts = torch.ones_like(logits_pos_concepts, dtype=torch.float32, device=device)\n",
    "    \n",
    "    batch_logits.append(logits_pos_concepts.flatten())\n",
    "    batch_labels.append(labels_pos_concepts.flatten())\n",
    "    \n",
    "    # print (\"\\nlogits_pos_concepts :\", logits_pos_concepts)\n",
    "    # print (\"labels :\", labels)\n",
    "    \n",
    "    loss_pos_concept = loss_fn(logits_pos_concepts, labels_pos_concepts)\n",
    "    # print (\"Loss positive concepts :\", loss_pos_concept)\n",
    "    \n",
    "    loss_neg_concept = 0.0\n",
    "    loss_neg_property = 0.0 \n",
    "    \n",
    "    for i in range(len(concept_id_list_for_batch)):\n",
    "        \n",
    "        if i < num_neg_concept:\n",
    "            \n",
    "            concept_id = concept_id_list_for_batch[i]\n",
    "            \n",
    "            # Extracting the property of the concept at the whole dataset level.\n",
    "            property_id_list_for_concept = torch.tensor(dataset.con_pro_dict[concept_id.item()], device=device)\n",
    "\n",
    "            # Extracting the negative property by excluding the properties that the concept may have at the  whole dataset level\n",
    "            negative_property_id_for_concept = torch.tensor([x for x in property_id_list_for_batch if x not in property_id_list_for_concept], device=device)\n",
    "\n",
    "            positive_property_for_concept_mask = torch.tensor([[1] if x in negative_property_id_for_concept else [0] for x in property_id_list_for_batch], device=device)\n",
    "\n",
    "            neg_property_embedding = torch.mul(property_embedding, positive_property_for_concept_mask)\n",
    "\n",
    "            concept_i_repeated = concept_embedding[i].unsqueeze(0).repeat(concept_embedding.shape[0], 1)\n",
    "\n",
    "            logits_neg_concepts = ((concept_i_repeated * neg_property_embedding).sum(-1).reshape(concept_i_repeated.shape[0], 1))\n",
    "\n",
    "            labels_neg_concepts = torch.zeros_like(logits_neg_concepts, dtype=torch.float32, device=device)\n",
    "\n",
    "            batch_logits.append(logits_neg_concepts.flatten())\n",
    "            batch_labels.append(labels_neg_concepts.flatten())\n",
    "            \n",
    "            loss_neg_concept += loss_fn(logits_neg_concepts, labels_neg_concepts)\n",
    "            \n",
    "            # print (loss_neg_concept)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            property_id = property_id_list_for_batch[i]\n",
    "            \n",
    "            # Extracting the concept for the property at the whole dataset level\n",
    "            concept_id_list_for_property = torch.tensor(dataset.prop_con_dict[property_id.item()], device=device)\n",
    "            \n",
    "            # Extracting the negative concepts for the property by excluding the concepts that the property may have at the whole dataset level.\n",
    "            negative_concept_id_for_property = torch.tensor([x for x in concept_id_list_for_batch if x not in concept_id_list_for_property], device=device)\n",
    "            \n",
    "            # [f(x) if condition else g(x) for x in sequence]\n",
    "            # mask i.e zero out the postive concept indices in concept_id_list_for_batch for this particular property id.\n",
    "            positive_concept_for_property_mask = torch.tensor([[1] if x in negative_concept_id_for_property else [0] for x in concept_id_list_for_batch], device=device)\n",
    "            \n",
    "            neg_concept_embedding = torch.mul(concept_embedding, positive_concept_for_property_mask)\n",
    "            \n",
    "            property_i_repeated = property_embedding[i].unsqueeze(0).repeat(property_embedding.shape[0], 1)\n",
    "            \n",
    "            logits_neg_property = ((neg_concept_embedding * property_i_repeated).sum(-1).reshape(neg_concept_embedding.shape[0], 1))\n",
    "            \n",
    "            labels_neg_property = torch.zeros_like(logits_neg_property, dtype=torch.float32, device=device)\n",
    "            \n",
    "            batch_logits.append(logits_neg_property.flatten())\n",
    "            batch_labels.append(labels_neg_property.flatten())\n",
    "            \n",
    "            loss_neg_property += loss_fn(logits_neg_property, labels_neg_property)\n",
    "    \n",
    "    \n",
    "#     print (\"\\t loss_pos_concept :\", loss_pos_concept, flush=True)\n",
    "#     print (\"\\t loss_neg_concept :\", loss_neg_concept, flush=True)\n",
    "#     print (\"\\t loss_neg_property :\", loss_neg_property, flush=True)\n",
    "    \n",
    "#     print (\"\\t Total Batch loss : \", loss_pos_concept + loss_neg_concept + loss_neg_property, flush=True)\n",
    "#     print ()\n",
    "#     print (\"batch_logits :\", batch_logits)\n",
    "#     print (\"batch_labels :\", batch_labels)\n",
    "    \n",
    "    batch_logits = torch.vstack(batch_logits).reshape(-1, 1)\n",
    "    batch_labels = torch.vstack(batch_labels).reshape(-1, 1)\n",
    "        \n",
    "    return loss_pos_concept + loss_neg_concept + loss_neg_property, batch_logits, batch_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925595b1-1da4-4f35-8e2c-ff9fc52f582a",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9960b4df-1105-4378-9b15-10a44a49de7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    epoch_loss = 0.0 \n",
    "    \n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dl):\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        concepts_batch, property_batch = train_dataset.add_context(batch)\n",
    "        \n",
    "        ids_dict = train_dataset.tokenize(concepts_batch, property_batch)\n",
    "\n",
    "        concept_inp_id, concept_attention_mask, property_input_id, property_attention_mask = [val.to(device) for _, val in ids_dict.items()]\n",
    "\n",
    "        concept_embedding, property_embedding, logits =  model(concept_input_id=concept_inp_id,\n",
    "                                                               concept_attention_mask=concept_attention_mask,\n",
    "                                                               property_input_id=property_input_id,\n",
    "                                                               property_attention_mask=property_attention_mask)\n",
    "\n",
    "        batch_loss, batch_logits, batch_labels = calculate_loss(train_dataset, batch, concept_embedding, property_embedding, logits, device=device)\n",
    "        \n",
    "        epoch_loss += batch_loss.item()\n",
    "        \n",
    "        batch_loss.backward()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / len(train_dl)\n",
    "    \n",
    "    return avg_epoch_loss\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ca9f8d-d6c7-4d82-b850-b4c10c888bf0",
   "metadata": {},
   "source": [
    "## Evaluate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67702c79-2119-4c38-8cef-9362b0dd4ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_logits, epoch_labels = [], []\n",
    "    \n",
    "    for step, batch in enumerate(valid_dl):\n",
    "        \n",
    "        concepts_batch, property_batch = valid_dataset.add_context(batch)\n",
    "        \n",
    "        ids_dict = valid_dataset.tokenize(concepts_batch, property_batch)\n",
    "\n",
    "        concept_inp_id, concept_attention_mask, property_input_id, property_attention_mask = [val.to(device) for _, val in ids_dict.items()]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            concept_embedding, property_embedding, logits =  model(concept_input_id=concept_inp_id,\n",
    "                                                               concept_attention_mask=concept_attention_mask,\n",
    "                                                               property_input_id=property_input_id,\n",
    "                                                               property_attention_mask=property_attention_mask)\n",
    "        \n",
    "        batch_loss, batch_logits, batch_labels = calculate_loss(valid_dataset, batch, concept_embedding, property_embedding, logits, device=device)\n",
    "        \n",
    "        epoch_logits.append(batch_logits)\n",
    "        epoch_labels.append(batch_labels)\n",
    "        \n",
    "        val_loss += batch_loss.item()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    epoch_logits = torch.round(torch.sigmoid(torch.vstack(epoch_logits))).reshape(-1, 1).detach().cpu().numpy()\n",
    "    epoch_labels = torch.vstack(epoch_labels).reshape(-1, 1).detach().cpu().numpy()\n",
    "\n",
    "    print (\"epoch_logits type: \", type(epoch_logits), flush=True)\n",
    "    print (\"epoch_logits shape :\", epoch_logits.shape, flush=True)\n",
    "    \n",
    "    print (\"epoch_labels type :\", type(epoch_labels), flush=True)\n",
    "    print (\"epoch_labels type :\", epoch_labels.shape, flush=True)\n",
    "    \n",
    "    scores = compute_scores(epoch_labels, epoch_logits)\n",
    "    \n",
    "    avg_val_loss = val_loss / len(valid_dl)\n",
    "    \n",
    "    return avg_val_loss, scores\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb2ad92-7042-422b-a602-77a76a40601d",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62147aa1-d97c-4315-a95f-aaeebcc02c18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_val_f1 = 0.0\n",
    "start_epoch = 1\n",
    "\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(start_epoch, num_epoch+1):\n",
    "    \n",
    "    train_loss = train()\n",
    "    val_loss, scores = evaluate()\n",
    "    \n",
    "    print (f\"\\n Epoch {epoch}\")\n",
    "    print(f\"Train Epoch : {epoch} ----> Train Average Epoch Loss : {train_loss}\", flush=True)\n",
    "    print(f\"Validation Epoch : {epoch} ----> Val Average Epoch Loss : {val_loss}\", flush=True)\n",
    "    print (f\"Best Validation F1 yet :\", best_val_f1)\n",
    "    \n",
    "    print (\"\\nValidation Metrices\")\n",
    "    \n",
    "    for key, val in scores.items():\n",
    "        print (f\"{key} : {val}\", flush=True)\n",
    "    \n",
    "    val_f1 = scores.get(\"binary_f1\")\n",
    "    \n",
    "    if val_f1 < best_val_f1:\n",
    "        patience_counter += 1 \n",
    "        \n",
    "    else:\n",
    "        patience_counter = 0\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    \n",
    "    if patience_counter > early_stopping_patience:\n",
    "        break\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be76222a-73d4-4cba-a929-f2d15dd55db7",
   "metadata": {},
   "source": [
    "## Test data class to load data for without in-batch negative sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b2e8d7-1955-4d59-827f-d2a746cea6d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_file_path):\n",
    "\n",
    "        self.data_df = pd.read_csv(data_file_path,\n",
    "            sep=\"\\t\",\n",
    "            header=None,\n",
    "            names=[\"concept\", \"property\", \"label\"],\n",
    "        )\n",
    "        \n",
    "        # self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"/scratch/c.scmag3/conceptEmbeddingModel/bertBaseUncasedPreTrained/tokenizer\")\n",
    "\n",
    "        self.context_num = context_num\n",
    "        self.label = self.data_df[\"label\"].values\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.data_df[\"concept\"][idx], self.data_df[\"property\"][idx]\n",
    "    \n",
    "    def add_context(self, batch):\n",
    "        \n",
    "        if self.context_num == 4:\n",
    "            \n",
    "            concept_context = \"Yesterday, I saw another \"\n",
    "            property_context = \"Yesterday, I saw a thing which is \"\n",
    "            \n",
    "            concepts_batch = [concept_context + x + \".\" for x in batch[0]]\n",
    "            property_batch = [property_context + x + \".\" for x in batch[1]]\n",
    "            \n",
    "        elif self.context_num == 1:\n",
    "            \n",
    "            concept_context = \"Concept : \"\n",
    "            property_context = \"Property : \"\n",
    "            \n",
    "            concepts_batch = [concept_context + x + \".\" for x in batch[0]]\n",
    "            property_batch = [property_context + x + \".\" for x in batch[1]]\n",
    "                    \n",
    "        elif self.context_num == 2:\n",
    "            \n",
    "            concept_context = \"The notion we are modelling : \"\n",
    "            property_context = \"The notion we are modelling : \"\n",
    "            \n",
    "            concepts_batch = [concept_context + x + \".\" for x in batch[0]]\n",
    "            property_batch = [property_context + x + \".\" for x in batch[1]]\n",
    "            \n",
    "            \n",
    "        elif self.context_num == 3:\n",
    "            \n",
    "            prefix_num = 5\n",
    "            suffix_num = 4  \n",
    "            \n",
    "            concepts_batch = [\"[MASK] \" * prefix_num + concept + \" \" + \"[MASK] \" * suffix_num + \".\" for concept in batch[0]]\n",
    "            property_batch = [\"[MASK] \" * prefix_num + prop + \" \" + \"[MASK] \" * suffix_num + \".\" for prop in batch[1]]\n",
    "            \n",
    "        \n",
    "        elif self.context_num == 5:\n",
    "            \n",
    "            concept_context = \"The notion we are modelling is called \"\n",
    "            property_context = \"The notion we are modelling is called \"\n",
    "            \n",
    "            concepts_batch = [concept_context + x + \".\" for x in batch[0]]\n",
    "            property_batch = [property_context + x + \".\" for x in batch[1]]\n",
    "                    \n",
    "        \n",
    "        return concepts_batch, property_batch\n",
    "        \n",
    "    def tokenize(self, concept_batch, property_batch, concept_max_len=64, property_max_len=64):\n",
    "        \n",
    "        concept_ids = self.tokenizer(\n",
    "            concept_batch,\n",
    "            max_length=concept_max_len,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        property_ids = self.tokenizer(\n",
    "            property_batch,\n",
    "            max_length=property_max_len,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "                \n",
    "        return {\"concept_inp_id\": concept_ids.get(\"input_ids\"),\n",
    "            \"concept_atten_mask\": concept_ids.get(\"attention_mask\"),\n",
    "            \"property_inp_id\": property_ids.get(\"input_ids\"),\n",
    "            \"property_atten_mask\": property_ids.get(\"attention_mask\")}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b641ff8f-5105-4b9a-b32a-3d3177b0fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data with 5 neg samples per concept-property pair\n",
    "\n",
    "file_test = \"65k_test_ms_concept_graph.tsv\"\n",
    "test_dataset = TestDataset(file_test)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=bs, sampler=test_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5be302-8402-4729-be06-a37dfd3d3ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_old_data(test_dataset, test_dl):\n",
    "    \n",
    "    print (\"Testing the model with old data 5 negatives\")\n",
    "    \n",
    "    best_model = best_model_path\n",
    "    \n",
    "    model = ConceptPropertyModel()\n",
    "    model.load_state_dict(torch.load(best_model))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    label = test_dataset.label\n",
    "    all_test_preds = [] \n",
    "    \n",
    "    for step, batch in enumerate(test_dl):\n",
    "        \n",
    "        concepts_batch, property_batch = test_dataset.add_context(batch)\n",
    "        \n",
    "        ids_dict = test_dataset.tokenize(concepts_batch, property_batch)\n",
    "\n",
    "        concept_inp_id, concept_attention_mask, property_input_id, property_attention_mask = [val.to(device) for _, val in ids_dict.items()]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            concept_embedding, property_embedding, logits =  model(concept_input_id=concept_inp_id,\n",
    "                                                               concept_attention_mask=concept_attention_mask,\n",
    "                                                               property_input_id=property_input_id,\n",
    "                                                               property_attention_mask=property_attention_mask)\n",
    "            \n",
    "        preds = torch.round(torch.sigmoid(logits))\n",
    "        all_test_preds.extend(preds.detach().cpu().numpy().flatten())\n",
    "            \n",
    "    \n",
    "    scores = compute_scores(label, all_test_preds)\n",
    "    \n",
    "    print (\"\\nTest Metrices\")\n",
    "    for key, val in scores.items():\n",
    "        print (f\"{key} : {val}\", flush=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b1c4c-fbf9-4658-a576-b741d2f0914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_old_data(test_dataset, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd6211a-fe1f-476d-80a5-64bd91a479aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test Data for in-batch negative sampling\n",
    "\n",
    "positive_only_test_file = \"mscg_test_pos.tsv\"\n",
    "\n",
    "test_nbs_dataset = ConceptPropertyDataset(positive_only_test_file)\n",
    "\n",
    "test_nbs_data_sampler = SequentialSampler(test_nbs_dataset)\n",
    "\n",
    "test_nbs_dl = torch.utils.data.DataLoader(test_nbs_dataset, batch_size=bs, sampler=test_nbs_data_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12601777-5bef-48d2-9bee-24f480d51b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nbs(test_nbs_dataset, test_nbs_dl):\n",
    "    \n",
    "    print ()\n",
    "    print (\"*\" * 50)\n",
    "    print (\"\\nTesting the model with Negative Batch sampling\")\n",
    "    \n",
    "    best_model = best_model_path\n",
    "    \n",
    "    model = ConceptPropertyModel()\n",
    "    model.load_state_dict(torch.load(best_model))\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    epoch_logits, epoch_labels = [], []\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    for step, batch in enumerate(test_nbs_dl):\n",
    "        \n",
    "        concepts_batch, property_batch = test_nbs_dataset.add_context(batch)\n",
    "        \n",
    "        ids_dict = test_nbs_dataset.tokenize(concepts_batch, property_batch)\n",
    "\n",
    "        concept_inp_id, concept_attention_mask, property_input_id, property_attention_mask = [val.to(device) for _, val in ids_dict.items()]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            concept_embedding, property_embedding, logits =  model(concept_input_id=concept_inp_id,\n",
    "                                                               concept_attention_mask=concept_attention_mask,\n",
    "                                                               property_input_id=property_input_id,\n",
    "                                                               property_attention_mask=property_attention_mask)\n",
    "        \n",
    "        batch_loss, batch_logits, batch_labels = calculate_loss(test_nbs_dataset, batch, concept_embedding, property_embedding, logits, device=device)\n",
    "        \n",
    "        epoch_logits.append(batch_logits)\n",
    "        epoch_labels.append(batch_labels)\n",
    "        \n",
    "        epoch_loss += batch_loss.item()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    epoch_logits = torch.round(torch.sigmoid(torch.vstack(epoch_logits))).reshape(-1, 1).detach().cpu().numpy()\n",
    "    epoch_labels = torch.vstack(epoch_labels).reshape(-1, 1).detach().cpu().numpy()\n",
    "\n",
    "    scores = compute_scores(epoch_labels, epoch_logits)\n",
    "    \n",
    "    for key, value in scores.items():\n",
    "        print (f\"{key} : {value}\", flush=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a0ca95-a6db-4956-bf8c-e41b98b8f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nbs(test_nbs_dataset, test_nbs_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca6aee2-b9b4-4001-8e75-aa0d6e8b2311",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
